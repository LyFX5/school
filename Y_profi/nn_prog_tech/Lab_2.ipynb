{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JLZlCZ0</td>\n",
       "      <td>Ep 1| Travelling through North East India | Of...</td>\n",
       "      <td>Tanya Khanijow\\n671K subscribers\\nSUBSCRIBE\\nT...</td>\n",
       "      <td>travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i9E_Blai8vk</td>\n",
       "      <td>Welcome to Bali | Travel Vlog | Priscilla Lee</td>\n",
       "      <td>Priscilla Lee\\n45.6K subscribers\\nSUBSCRIBE\\n*...</td>\n",
       "      <td>travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r284c-q8oY</td>\n",
       "      <td>My Solo Trip to ALASKA | Cruising From Vancouv...</td>\n",
       "      <td>Allison Anderson\\n588K subscribers\\nSUBSCRIBE\\...</td>\n",
       "      <td>travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Qmi-Xwq-ME</td>\n",
       "      <td>Traveling to the Happiest Country in the World!!</td>\n",
       "      <td>Yes Theory\\n6.65M subscribers\\nSUBSCRIBE\\n*BLA...</td>\n",
       "      <td>travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>_lcOX55Ef70</td>\n",
       "      <td>Solo in Paro Bhutan | Tiger's Nest visit | Bhu...</td>\n",
       "      <td>Tanya Khanijow\\n671K subscribers\\nSUBSCRIBE\\nH...</td>\n",
       "      <td>travel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          link                                              title  \\\n",
       "0      JLZlCZ0  Ep 1| Travelling through North East India | Of...   \n",
       "1  i9E_Blai8vk      Welcome to Bali | Travel Vlog | Priscilla Lee   \n",
       "2   r284c-q8oY  My Solo Trip to ALASKA | Cruising From Vancouv...   \n",
       "3   Qmi-Xwq-ME   Traveling to the Happiest Country in the World!!   \n",
       "4  _lcOX55Ef70  Solo in Paro Bhutan | Tiger's Nest visit | Bhu...   \n",
       "\n",
       "                                         description category  \n",
       "0  Tanya Khanijow\\n671K subscribers\\nSUBSCRIBE\\nT...   travel  \n",
       "1  Priscilla Lee\\n45.6K subscribers\\nSUBSCRIBE\\n*...   travel  \n",
       "2  Allison Anderson\\n588K subscribers\\nSUBSCRIBE\\...   travel  \n",
       "3  Yes Theory\\n6.65M subscribers\\nSUBSCRIBE\\n*BLA...   travel  \n",
       "4  Tanya Khanijow\\n671K subscribers\\nSUBSCRIBE\\nH...   travel  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_meta = pd.read_csv('youtube.csv')\n",
    "video_meta.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories: travel, food, art_music, history\n",
      "Average text len: 419.2570158377327\n"
     ]
    }
   ],
   "source": [
    "# Categories\n",
    "print(\"Categories:\", \", \".join(video_meta['category'].unique()))\n",
    "\n",
    "# Average text length\n",
    "print(\"Average text len:\", np.average([len(desc) for desc in video_meta['description']]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
       "        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  40,\n",
       "        41,  42,  44,  45,  46,  49,  51,  52,  54,  55,  56,  57,  58,\n",
       "        61,  63,  68,  70,  71,  75,  81,  82,  91,  94,  95,  96,  97,\n",
       "        99, 107, 108, 122, 139, 145, 179, 218, 262])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique([desc.count('\\n') for desc in video_meta['description']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Tanya Khanijow\\n671K subscribers\\nSUBSCRIBE\\nT...\n",
       "1       Priscilla Lee\\n45.6K subscribers\\nSUBSCRIBE\\n*...\n",
       "2       Allison Anderson\\n588K subscribers\\nSUBSCRIBE\\...\n",
       "3       Yes Theory\\n6.65M subscribers\\nSUBSCRIBE\\n*BLA...\n",
       "4       Tanya Khanijow\\n671K subscribers\\nSUBSCRIBE\\nH...\n",
       "                              ...                        \n",
       "3594    CrashCourse\\n12.4M subscribers\\nSUBSCRIBE\\nThe...\n",
       "3595    Publications Office of the European Union\\n3.2...\n",
       "3596    History Time\\n619K subscribers\\nSUBSCRIBE\\n- W...\n",
       "3597    Mr. Raymond's Civics and Social Studies Academ...\n",
       "3598    Paul Sargent\\n25.3K subscribers\\nSUBSCRIBE\\nIn...\n",
       "Name: description, Length: 3599, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_meta['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_pattern = \"^(?P<author>.+?)(?:\\\\n(?P<sub_count>[\\d\\.]+[MK]?)\\ssubscribers)?(?:\\\\nJOIN)?\\\\nSUBSCRIBE(?:\\\\n(?P<text>.+?))?(?:\\\\nSHOW\\sMORE)?$\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k9/xzf3np7563ggyh9sqb6zzk8m0000gn/T/ipykernel_96018/4010008201.py:6: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  data['sub_count'] = data['sub_count'].str.replace('K', '000').str.replace('M', '000000').str.replace('.', '').astype(pd.Int64Dtype())\n",
      "/var/folders/k9/xzf3np7563ggyh9sqb6zzk8m0000gn/T/ipykernel_96018/4010008201.py:7: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  data['title'] = data['title'].str.replace('|', '')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Concat columns\n",
    "extracted = video_meta['description'].str.extract(description_pattern, expand=True, flags=re.MULTILINE | re.DOTALL)\n",
    "data = video_meta.join(extracted)\n",
    "data['sub_count'] = data['sub_count'].str.replace('K', '000').str.replace('M', '000000').str.replace('.', '').astype(pd.Int64Dtype())\n",
    "data['title'] = data['title'].str.replace('|', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyyoutube import Api\n",
    "\n",
    "api = Api(api_key='AIzaSyCKTFpwCzPH_sKtSO9xv5kulKef0NhNW4k')\n",
    "\n",
    "def get_videos(video_id):\n",
    "    return api.get_video_by_id(video_id=video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching views:  40%|███▉      | 1427/3599 [02:31<03:38,  9.93it/s]/usr/local/lib/python3.10/site-packages/dataclasses_json/core.py:171: RuntimeWarning: `NoneType` object value of non-optional type blocked detected when decoding RegionRestriction.\n",
      "  warnings.warn(f\"`NoneType` object {warning}.\", RuntimeWarning)\n",
      "Fetching views: 100%|██████████| 3599/3599 [06:22<00:00,  9.42it/s]\n"
     ]
    }
   ],
   "source": [
    "videos = []\n",
    "for link in tqdm(data['link'], desc=\"Fetching views\"):\n",
    "   videos.append(get_videos(link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3599/3599 [00:00<00:00, 568904.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# data['views'] = [video.items[0].statistics.viewCount for video in videos]\n",
    "views = []\n",
    "likes = []\n",
    "comments = []\n",
    "for video in tqdm(videos):\n",
    "  try:\n",
    "    view = video.items[0].statistics.viewCount\n",
    "    like = video.items[0].statistics.likeCount\n",
    "    comment = video.items[0].statistics.commentCount\n",
    "    views.append(view)\n",
    "    likes.append(like)\n",
    "    comments.append(comment)\n",
    "  except:\n",
    "    views.append(None)\n",
    "    likes.append(None)\n",
    "    comments.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['views'] = pd.array(views, dtype=pd.Int64Dtype())\n",
    "data['likes'] = pd.array(likes, dtype=pd.Int64Dtype())\n",
    "data['comments'] = pd.array(comments, dtype=pd.Int64Dtype())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "# pkl.dump(data, open('data.pkl', 'wb'))\n",
    "data = pkl.load(open('data.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['title'] = data['title'].str.lower()\n",
    "data['text'] = data['text'].str.lower()\n",
    "data.to_csv('data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "loading file spiece.model from cache at /Users/sdfedorov/.cache/huggingface/hub/models--albert-base-v2/snapshots/51dbd9db43a0c6eba97f74b91ce26fface509e0b/spiece.model\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /Users/sdfedorov/.cache/huggingface/hub/models--albert-base-v2/snapshots/51dbd9db43a0c6eba97f74b91ce26fface509e0b/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"albert-base-v2\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /Users/sdfedorov/.cache/huggingface/hub/models--albert-base-v2/snapshots/51dbd9db43a0c6eba97f74b91ce26fface509e0b/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"output_attentions\": true,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/sdfedorov/.cache/huggingface/hub/models--albert-base-v2/snapshots/51dbd9db43a0c6eba97f74b91ce26fface509e0b/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing AlbertForPreTraining.\n",
      "\n",
      "Some weights of AlbertForPreTraining were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['sop_classifier.classifier.weight', 'sop_classifier.classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import *\n",
    "import torch\n",
    "\n",
    "tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "model = AlbertForPreTraining.from_pretrained(\"albert-base-v2\", output_hidden_states=True, output_attentions=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/sdfedorov/.cache/huggingface/hub/models--albert-base-v2/snapshots/51dbd9db43a0c6eba97f74b91ce26fface509e0b/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"output_attentions\": true,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/sdfedorov/.cache/huggingface/hub/models--albert-base-v2/snapshots/51dbd9db43a0c6eba97f74b91ce26fface509e0b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of AlbertModel were initialized from the model checkpoint at albert-base-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = AlbertModel.from_pretrained(\"albert-base-v2\", output_hidden_states=True, output_attentions=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Dec 15 2022, 18:25:35) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
