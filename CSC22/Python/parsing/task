

Как уже говорилось, мы стремимся разрабатывать прикладные, общие решения, снабжая инструментами для работы.

Мы затронем тему формирования датасетов из интернет-источников. Как я уже говорил, это довольно заметная тема рынка IT-решений, начиная от чего-то наколеночного до собственных кластерных решений и сервисов.

Приведу пример чего-то из серой зоны бирж фриланса:

    Требуется создать парсер для поиска подозрительно низких цен на запчасти у поставщиков автозапчастей Автопитер или Emex. Он должен искать "Discount" цены по номерам запчастей из подгруженной общей базы. Скорость обработки должна быть высокая.

    Возможные трудности: у данных поставщиков имеются различного вида защиты от парсинга которые нужно будет обойти.

Если говорить про решения-сервисы, можно взять за основу список на Хабре: https://habr.com/ru/post/543760/.

Немного ссылок оттуда (только для примера, мы пишем собственные программы, конечно):

    https://www.scraperapi.com/
    https://www.octoparse.com/
    https://data-ox.com/
    https://www.scraping-bot.io/
    https://www.wintr.com/
    https://www.parsehub.com/
    https://www.diffbot.com/

У многих есть обучающие видео: https://www.youtube.com/watch?v=F4VbwAuwe0I&t=221s

Естественно, если поставить перед вами цель распарсить что-либо с конкретной html-страницы, вы легко это сделаете. Давайте сделаем упор на создание общих инструментов. Решим 2 классические задачи парсинга. Вы можете воспользоваться алгоритмом-подсказкой или сделать что-то свое. Важное условие. Ваше решение не должно зависеть от конкретной верстки насколько это возможно.
1. Основное содержимое

Если мы заглянем на любой портал, то основное содержимое новости или статьи, как правило, содержится в обрамлении меню, рекламы, дополнительных блоков, ссылок и т.д. Существует множество плагинов, которые умеют делать так называемый "режим чтения", убирая все, кроме основного текста.

Давайте попробуем сделать подобную программу. На вход подается url страницы, а на выход -- основной текст.

Попробуем сделать сначала мини-задачу.
1.1 Процент текста на странице.

Просто для какого-либо сайта оцените процент чистого текста к размеру всей страницы, включая html-теги. Полезной информации будет около 10% или меньше.
1.2 Алгоритм поиска

Попробуйте в дереве html-документа найти наивысший узел (близкий к корню), у которого соотношение чистый текст / html близко к какой-нибудь эмперической границе, например, 0.9. Подберите его для пары сайтов экспериментально.
2. Табличные данные

На сайте, например, магазина, многие результаты поиска графически оформлены как таблица или список. Хочется получить такую таблицу для дальнейшей автоматической обработки.

Ни в коем случае не надо думать, что будут использоваться именно теги table. Путь может быть чем-то таким: /div/main/div/div/div/div/div/div/div/div.
Алгоритм

Если есть таблица или список, она регулярная, то есть все элементы содержатся в одинаковом обрамлении. Это значит, что можно просто сделать поиск одинаковых путей / поддеревьев, если считать от корня.

Например, если мы упорядочим пути от корня по частоте:

/div/main/div/div/div/div/div/div/div/div 105
/div/main/div/div/div/div/div/div/div/div/div 105
/div/main/div/div/div/div/div/div/div/div/div/span/span/span/a/div/div/span 104
/div/main/div/div/div/div/div/div/div/div/div/span 103
/div/main/div/div/div/div/div/div/div/div/div/span/span 103
/div/main/div/div/div/div/div/div/div/div/div/span/span/span 103
/div/main/div/div/div/div/div/div/div/div/div/span/span/span/a 103

Явно, что /div/main/div/div/div/div/div/div/div/div -- это список / таблица из 105 элементов, внутри которой существуют под-теги, в которых уже каждый элемент расписан на, например, заголовок, цену и т.д. Их можно сохранять отдельно в виде словаря. То есть на выходе нужен список из словарей.
P.S.

Сайты, с которыми вы будет экспериментировать, определите сами. Выбрать надо такие, которые отдают контент без дополнительных условий в виде правильных заголовков, cookies, капчи и т.д.

Желательно использовать стандартную библиотеку. Желательно сделать основной каркас в виде классов, используя ООП с вкраплениями функционального программирования как синтаксического сахара и рекурсии по необходимости.

Сдавать ДЗ нужно будет в репозиторий, координаты появятся тут чуть позже.
