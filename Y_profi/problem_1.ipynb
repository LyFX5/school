{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-07T15:26:40.967922Z",
     "start_time": "2023-04-07T15:26:21.048110Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3 1 2\n"
     ]
    }
   ],
   "source": [
    "n = int(input())\n",
    "a = list(map(int, input().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-07T15:26:41.668747Z",
     "start_time": "2023-04-07T15:26:41.661463Z"
    }
   },
   "outputs": [],
   "source": [
    "def solve(a, left, right, dp):\n",
    "    if dp is None:\n",
    "        dp = {}\n",
    "    if left is None:\n",
    "        left = 0\n",
    "    if right is None:\n",
    "        right = len(a) - 1\n",
    "\n",
    "    if (left, right, a[left], a[right]) in dp:\n",
    "        return dp[(left, right, a[left], a[right])]\n",
    "\n",
    "    q = []\n",
    "    for i in range(left, right):\n",
    "        g = min(a[i], a[i+1])\n",
    "        if g > 0:\n",
    "            a[i] -= g\n",
    "            a[i+1] -= g\n",
    "\n",
    "            q.append(solve(a, left, i, dp) ^ solve(a, i+1, right, dp))\n",
    "\n",
    "            a[i] += g\n",
    "            a[i+1] += g\n",
    "    q = set(q)\n",
    "\n",
    "    mex = 0\n",
    "    while mex in q:\n",
    "        mex += 1\n",
    "    dp[(left, right, a[left], a[right])] = mex\n",
    "\n",
    "    return mex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-07T15:26:42.559367Z",
     "start_time": "2023-04-07T15:26:42.549919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(solve(a, 0, len(a)-1, {}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-07T15:26:47.975311Z",
     "start_time": "2023-04-07T15:26:47.945934Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "13 ^ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [6]\u001B[0m, in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m lmbd \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.1\u001B[39m\n\u001B[1;32m      4\u001B[0m degree_list \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m12\u001B[39m, \u001B[38;5;241m13\u001B[39m]\n\u001B[0;32m----> 5\u001B[0m cmap \u001B[38;5;241m=\u001B[39m \u001B[43mplt\u001B[49m\u001B[38;5;241m.\u001B[39mget_cmap(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mjet\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      6\u001B[0m colors \u001B[38;5;241m=\u001B[39m [cmap(i) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m np\u001B[38;5;241m.\u001B[39mlinspace(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mlen\u001B[39m(degree_list))]\n\u001B[1;32m      8\u001B[0m margin \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.3\u001B[39m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# define regularization parameter\n",
    "lmbd = 0.1\n",
    "\n",
    "degree_list = [1, 2, 3, 10, 12, 13]\n",
    "cmap = plt.get_cmap('jet')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, len(degree_list))]\n",
    "\n",
    "margin = 0.3\n",
    "plt.plot(data['support'], data['values'], 'b--', alpha=0.5, label='manifold')\n",
    "plt.scatter(data['x_train'], data['y_train'], 40, 'g', 'o', alpha=0.8, label='data')\n",
    "\n",
    "w_list_l2 = []\n",
    "err = []\n",
    "for ix, degree in enumerate(degree_list):\n",
    "    dlist = [[1]*data['x_train'].shape[0]] + map(lambda n: data['x_train']**n, range(1, degree + 1))\n",
    "    X = np.array(dlist).T\n",
    "    w = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X) + lmbd*np.eye(X.shape[1])), X.T), data['y_train'])\n",
    "    w_list_l2.append((degree, w))\n",
    "    y_hat = np.dot(w, X.T)\n",
    "    plt.plot(data['x_train'], y_hat, color=colors[ix], label='poly degree: %i' % degree)\n",
    "    err.append(np.mean((data['y_train'] - y_hat)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [7]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mensemble\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RandomForestClassifier\n\u001B[1;32m      2\u001B[0m forest \u001B[38;5;241m=\u001B[39m RandomForestClassifier(n_estimators\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m200\u001B[39m, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m17\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m forest\u001B[38;5;241m.\u001B[39mfit(\u001B[43mX_train\u001B[49m, y_train)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mround\u001B[39m(forest\u001B[38;5;241m.\u001B[39mscore(X_test, y_test), \u001B[38;5;241m3\u001B[39m))\n",
      "\u001B[0;31mNameError\u001B[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=17)\n",
    "forest.fit(X_train, y_train)\n",
    "print(round(forest.score(X_test, y_test), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mla'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[0;32mIn [8]\u001B[0m, in \u001B[0;36m<cell line: 11>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m make_classification\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m make_regression\n\u001B[0;32m---> 11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmla\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlinear_models\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LinearRegression, LogisticRegression\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmla\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m mean_squared_error, accuracy\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# Change to DEBUG to see convergence\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'mla'"
     ]
    }
   ],
   "source": [
    "# linear models \n",
    "import logging\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except ImportError:\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "from mla.linear_models import LinearRegression, LogisticRegression\n",
    "from mla.metrics.metrics import mean_squared_error, accuracy\n",
    "\n",
    "# Change to DEBUG to see convergence\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "\n",
    "def regression():\n",
    "    # Generate a random regression problem\n",
    "    X, y = make_regression(\n",
    "        n_samples=10000, n_features=100, n_informative=75, n_targets=1, noise=0.05, random_state=1111, bias=0.5\n",
    "    )\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1111)\n",
    "\n",
    "    model = LinearRegression(lr=0.01, max_iters=2000, penalty=\"l2\", C=0.03)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    print(\"regression mse\", mean_squared_error(y_test, predictions))\n",
    "\n",
    "\n",
    "def classification():\n",
    "    # Generate a random binary classification problem.\n",
    "    X, y = make_classification(\n",
    "        n_samples=1000, n_features=100, n_informative=75, random_state=1111, n_classes=2, class_sep=2.5\n",
    "    )\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1111)\n",
    "\n",
    "    model = LogisticRegression(lr=0.01, max_iters=500, penalty=\"l1\", C=0.01)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    print(\"classification accuracy\", accuracy(y_test, predictions))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    regression()\n",
    "    classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mla'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[0;32mIn [9]\u001B[0m, in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m make_blobs\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmla\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkmeans\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m KMeans\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mkmeans_example\u001B[39m(plot\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m      9\u001B[0m     X, y \u001B[38;5;241m=\u001B[39m make_blobs(centers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m, n_samples\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m500\u001B[39m, n_features\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'mla'"
     ]
    }
   ],
   "source": [
    "# KMeans\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from mla.kmeans import KMeans\n",
    "\n",
    "\n",
    "def kmeans_example(plot=False):\n",
    "    X, y = make_blobs(centers=4, n_samples=500, n_features=2, shuffle=True, random_state=42)\n",
    "    clusters = len(np.unique(y))\n",
    "    k = KMeans(K=clusters, max_iters=150, init=\"++\")\n",
    "    k.fit(X)\n",
    "    k.predict()\n",
    "\n",
    "    if plot:\n",
    "        k.plot()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    kmeans_example(plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mla'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[0;32mIn [10]\u001B[0m, in \u001B[0;36m<cell line: 6>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m roc_auc_score\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m train_test_split\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmla\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnaive_bayes\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m NaiveBayesClassifier\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mclassification\u001B[39m():\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;66;03m# Generate a random binary classification problem.\u001B[39;00m\n\u001B[1;32m     11\u001B[0m     X, y \u001B[38;5;241m=\u001B[39m make_classification(\n\u001B[1;32m     12\u001B[0m         n_samples\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m, n_features\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, n_informative\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1111\u001B[39m, n_classes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, class_sep\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2.5\u001B[39m, n_redundant\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     13\u001B[0m     )\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'mla'"
     ]
    }
   ],
   "source": [
    "# naive bayes\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from mla.naive_bayes import NaiveBayesClassifier\n",
    "\n",
    "\n",
    "def classification():\n",
    "    # Generate a random binary classification problem.\n",
    "    X, y = make_classification(\n",
    "        n_samples=1000, n_features=10, n_informative=10, random_state=1111, n_classes=2, class_sep=2.5, n_redundant=0\n",
    "    )\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1111)\n",
    "\n",
    "    model = NaiveBayesClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)[:, 1]\n",
    "\n",
    "    print(\"classification accuracy\", roc_auc_score(y_test, predictions))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mla'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[0;32mIn [11]\u001B[0m, in \u001B[0;36m<cell line: 10>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m make_regression\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspatial\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m distance\n\u001B[0;32m---> 10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmla\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m knn\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmla\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m mean_squared_error, accuracy\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mregression\u001B[39m():\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;66;03m# Generate a random regression problem\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'mla'"
     ]
    }
   ],
   "source": [
    "# nearest_neighbors\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except ImportError:\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets import make_regression\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from mla import knn\n",
    "from mla.metrics.metrics import mean_squared_error, accuracy\n",
    "\n",
    "\n",
    "def regression():\n",
    "    # Generate a random regression problem\n",
    "    X, y = make_regression(\n",
    "        n_samples=500, n_features=5, n_informative=5, n_targets=1, noise=0.05, random_state=1111, bias=0.5\n",
    "    )\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1111)\n",
    "\n",
    "    model = knn.KNNRegressor(k=5, distance_func=distance.euclidean)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    print(\"regression mse\", mean_squared_error(y_test, predictions))\n",
    "\n",
    "\n",
    "def classification():\n",
    "    X, y = make_classification(\n",
    "        n_samples=500,\n",
    "        n_features=5,\n",
    "        n_informative=5,\n",
    "        n_redundant=0,\n",
    "        n_repeated=0,\n",
    "        n_classes=3,\n",
    "        random_state=1111,\n",
    "        class_sep=1.5,\n",
    "    )\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1111)\n",
    "\n",
    "    clf = knn.KNNClassifier(k=5, distance_func=distance.euclidean)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "    print(\"classification accuracy\", accuracy(y_test, predictions))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    regression()\n",
    "    classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mla'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[0;32mIn [12]\u001B[0m, in \u001B[0;36m<cell line: 14>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcross_validation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m train_test_split\n\u001B[0;32m---> 14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmla\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mensemble\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrandom_forest\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RandomForestClassifier, RandomForestRegressor\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmla\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m mean_squared_error\n\u001B[1;32m     17\u001B[0m logging\u001B[38;5;241m.\u001B[39mbasicConfig(level\u001B[38;5;241m=\u001B[39mlogging\u001B[38;5;241m.\u001B[39mDEBUG)\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'mla'"
     ]
    }
   ],
   "source": [
    "# random forest\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except ImportError:\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from mla.ensemble.random_forest import RandomForestClassifier, RandomForestRegressor\n",
    "from mla.metrics.metrics import mean_squared_error\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "\n",
    "def classification():\n",
    "    # Generate a random binary classification problem.\n",
    "    X, y = make_classification(\n",
    "        n_samples=500, n_features=10, n_informative=10, random_state=1111, n_classes=2, class_sep=2.5, n_redundant=0\n",
    "    )\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=1111)\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=10, max_depth=4)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    predictions_prob = model.predict(X_test)[:, 1]\n",
    "    predictions = np.argmax(model.predict(X_test), axis=1)\n",
    "    #print(predictions.shape)\n",
    "    print(\"classification, roc auc score: %s\" % roc_auc_score(y_test, predictions_prob))\n",
    "    print(\"classification, accuracy score: %s\" % accuracy_score(y_test, predictions))\n",
    "\n",
    "\n",
    "def regression():\n",
    "    # Generate a random regression problem\n",
    "    X, y = make_regression(\n",
    "        n_samples=500, n_features=5, n_informative=5, n_targets=1, noise=0.05, random_state=1111, bias=0.5\n",
    "    )\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1111)\n",
    "\n",
    "    model = RandomForestRegressor(n_estimators=50, max_depth=10, max_features=3)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    print(\"regression, mse: %s\" % mean_squared_error(y_test.flatten(), predictions.flatten()))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    classification()\n",
    "    # regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mla'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[0;32mIn [13]\u001B[0m, in \u001B[0;36m<cell line: 10>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcross_validation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m train_test_split\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m make_classification\n\u001B[0;32m---> 10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmla\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m accuracy\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmla\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msvm\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkernerls\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Linear, RBF\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmla\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msvm\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msvm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SVM\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'mla'"
     ]
    }
   ],
   "source": [
    "# svm\n",
    "import logging\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except ImportError:\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from mla.metrics.metrics import accuracy\n",
    "from mla.svm.kernerls import Linear, RBF\n",
    "from mla.svm.svm import SVM\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "\n",
    "def classification():\n",
    "    # Generate a random binary classification problem.\n",
    "    X, y = make_classification(\n",
    "        n_samples=1200, n_features=10, n_informative=5, random_state=1111, n_classes=2, class_sep=1.75\n",
    "    )\n",
    "    # Convert y to {-1, 1}\n",
    "    y = (y * 2) - 1\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1111)\n",
    "\n",
    "    for kernel in [RBF(gamma=0.1), Linear()]:\n",
    "        model = SVM(max_iter=500, kernel=kernel, C=0.6)\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        print(\"Classification accuracy (%s): %s\" % (kernel, accuracy(y_test, predictions)))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mla'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[0;32mIn [14]\u001B[0m, in \u001B[0;36m<cell line: 6>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m datasets\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmla\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkmeans\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m KMeans\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmla\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgaussian_mixture\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m GaussianMixture\n\u001B[1;32m      9\u001B[0m random\u001B[38;5;241m.\u001B[39mseed(\u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'mla'"
     ]
    }
   ],
   "source": [
    "# gausian mixture\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from mla.kmeans import KMeans\n",
    "from mla.gaussian_mixture import GaussianMixture\n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(6)\n",
    "\n",
    "\n",
    "def make_clusters(skew=True, *arg, **kwargs):\n",
    "    X, y = datasets.make_blobs(*arg, **kwargs)\n",
    "    if skew:\n",
    "        nrow = X.shape[1]\n",
    "        for i in np.unique(y):\n",
    "            X[y == i] = X[y == i].dot(np.random.random((nrow, nrow)) - 0.5)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def KMeans_and_GMM(K):\n",
    "    COLOR = \"bgrcmyk\"\n",
    "\n",
    "    X, y = make_clusters(skew=True, n_samples=1500, centers=K)\n",
    "    _, axes = plt.subplots(1, 3)\n",
    "\n",
    "    # Ground Truth\n",
    "    axes[0].scatter(X[:, 0], X[:, 1], c=[COLOR[int(assignment)] for assignment in y])\n",
    "    axes[0].set_title(\"Ground Truth\")\n",
    "\n",
    "    # KMeans\n",
    "    kmeans = KMeans(K=K, init=\"++\")\n",
    "    kmeans.fit(X)\n",
    "    kmeans.predict()\n",
    "    axes[1].set_title(\"KMeans\")\n",
    "    kmeans.plot(ax=axes[1], holdon=True)\n",
    "\n",
    "    # Gaussian Mixture\n",
    "    gmm = GaussianMixture(K=K, init=\"kmeans\")\n",
    "    gmm.fit(X)\n",
    "    axes[2].set_title(\"Gaussian Mixture\")\n",
    "    gmm.plot(ax=axes[2])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    KMeans_and_GMM(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mla'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[0;32mIn [15]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# deep q learning\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mlogging\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmla\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mneuralnet\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m NeuralNet\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmla\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mneuralnet\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Activation, Dense\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmla\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mneuralnet\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moptimizers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Adam\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'mla'"
     ]
    }
   ],
   "source": [
    "# deep q learning\n",
    "import logging\n",
    "\n",
    "from mla.neuralnet import NeuralNet\n",
    "from mla.neuralnet.layers import Activation, Dense\n",
    "from mla.neuralnet.optimizers import Adam\n",
    "from mla.rl.dqn import DQN\n",
    "\n",
    "logging.basicConfig(level=logging.CRITICAL)\n",
    "\n",
    "\n",
    "def mlp_model(n_actions, batch_size=64):\n",
    "    model = NeuralNet(\n",
    "        layers=[Dense(32), Activation(\"relu\"), Dense(n_actions)],\n",
    "        loss=\"mse\",\n",
    "        optimizer=Adam(),\n",
    "        metric=\"mse\",\n",
    "        batch_size=batch_size,\n",
    "        max_epochs=1,\n",
    "        verbose=False,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model = DQN(n_episodes=2500, batch_size=64)\n",
    "model.init_environment(\"CartPole-v0\")\n",
    "model.init_model(mlp_model)\n",
    "\n",
    "try:\n",
    "    # Train the model\n",
    "    # It can take from 300 to 2500 episodes to solve CartPole-v0 problem due to randomness of environment.\n",
    "    # You can stop training process using Ctrl+C signal\n",
    "    # Read more about this problem: https://gym.openai.com/envs/CartPole-v0\n",
    "    model.train(render=False)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "# Render trained model\n",
    "model.play(episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c conda-forge mlapack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
