{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e64b24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f448799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO suturation action, reward state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9368a0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNetwork(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(DNetwork, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        hiden_layer_len_1 = 50\n",
    "#         hiden_layer_len_2 = 50\n",
    "        self.hidden_1 = nn.Linear(self.in_dim, hiden_layer_len_1, bias=True)\n",
    "#         self.hidden_2 = nn.Linear(hiden_layer_len_1, hiden_layer_len_2, bias=True)\n",
    "        self.output = nn.Linear(hiden_layer_len_1, self.out_dim, bias=True)\n",
    "        self.activation_on_hidden_1 =  nn.ReLU() # nn.Tanh() # nn.ReLU()\n",
    "#         self.activation_on_hidden_2 = nn.ReLU() #\n",
    "        self.activation_on_output = nn.ReLU() # nn.Tanh()  # nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(x)\n",
    "        x = self.hidden_1(x)\n",
    "#         print(x)\n",
    "        x = self.activation_on_hidden_1(x)\n",
    "#         print(x)\n",
    "#         x = self.hidden_2(x)\n",
    "#         x = self.activation_on_hidden_2(x)\n",
    "        x = self.output(x)\n",
    "#         print(x)\n",
    "        x = self.activation_on_output(x)\n",
    "#         print(x)\n",
    "        # x = self.Norm(x, self.sigma*torch.eye(2)).sample()\n",
    "        # torch.maximum(torch.minimum(x, torch.tensor(1)), torch.tensor(-1)) # self.activation_on_output(x) #\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fe4d7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utils:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def prepare_state_action(self, state: list, action: float) -> np.ndarray:\n",
    "        s = np.array(state)\n",
    "#         s = s.reshape((s.shape[0], 1))\n",
    "        a = np.array([action])\n",
    "#         a = a.reshape((a.shape[0], 1))\n",
    "        return np.concatenate((s, a), axis=0)\n",
    "\n",
    "    def prepare_state(self, state: list) -> np.ndarray:\n",
    "        s = np.array(state)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3add55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils = Utils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4772c1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = [-0.42193421721458435, -0.015489596903575209, 0.004222722724080086]\n",
    "action = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "495e04a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dynamics:\n",
    "    def __init__(self, state_dim=3, action_dim=1):\n",
    "        self.model = DNetwork(in_dim=(state_dim+action_dim), out_dim=state_dim)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(params=self.model.parameters(), lr=0.002)\n",
    "    \n",
    "    def step(self, state: list, action: float) -> list:\n",
    "        s_a = utils.prepare_state_action(state, action)\n",
    "        with torch.no_grad():\n",
    "            state_next = self.model(torch.from_numpy(s_a).float()) # .detach().numpy()\n",
    "        # list(map(lambda x: x[0], state_next.detach().numpy()))\n",
    "        state_next = list(state_next.detach().numpy())\n",
    "        return state_next\n",
    "    \n",
    "    def fit(self, states_actions: np.ndarray, next_states: np.ndarray):\n",
    "        states_actions = torch.from_numpy(states_actions).float()\n",
    "        next_states = torch.from_numpy(next_states).float()\n",
    "        self.optimizer.zero_grad()\n",
    "        guessies = self.model(states_actions)\n",
    "        loss = self.criterion(guessies, next_states)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41ec12db",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamics = Dynamics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00a37291",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = dynamics.step(state, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7b4bc601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.123333536, 0.0]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc919770",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def __init__(self, state_dim=3, action_dim=1):\n",
    "        self.model = DNetwork(in_dim=state_dim, out_dim=action_dim)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(params=self.model.parameters(), lr=0.002)\n",
    "        \n",
    "    def get_action(self, state: list) -> float:\n",
    "        s = utils.prepare_state(state)\n",
    "        with torch.no_grad():\n",
    "            action = self.model(torch.from_numpy(s).float()) # .detach().numpy()\n",
    "        action = action.detach().numpy()[0]\n",
    "        return action\n",
    "    \n",
    "    def fit(self, states: np.ndarray, actions: np.ndarray):\n",
    "        states = torch.from_numpy(states).float()\n",
    "        actions = torch.from_numpy(actions).float()\n",
    "        self.optimizer.zero_grad()\n",
    "        guessies = self.model(states)\n",
    "        loss = self.criterion(guessies, actions)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f6ec882f",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "88dfa587",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = policy.get_action(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f282596f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af57c61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reward:\n",
    "    def __init__(self, state_dim=3, action_dim=1):\n",
    "        self.model = DNetwork(in_dim=(state_dim+action_dim), out_dim=1)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(params=self.model.parameters(), lr=0.002)\n",
    "    \n",
    "    def calculate(self, state: list, action: float) -> float:\n",
    "        s_a = utils.prepare_state_action(state, action)\n",
    "        with torch.no_grad():\n",
    "            reward = self.model(torch.from_numpy(s_a).float()) # .detach().numpy()\n",
    "        reward = reward.detach().numpy()[0]\n",
    "        return reward\n",
    "    \n",
    "    def fit(self, states_actions: np.ndarray, rewards: np.ndarray):\n",
    "        states_actions = torch.from_numpy(states_actions).float()\n",
    "        rewards = torch.from_numpy(rewards).float()\n",
    "        self.optimizer.zero_grad()\n",
    "        guessies = self.model(states_actions)\n",
    "        loss = self.criterion(guessies, rewards)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a069fe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_function = Reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0aaa500",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = reward_function.calculate(state, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66ff4f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15028724"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f02b4d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy()\n",
    "reward_function = Reward()\n",
    "dynamics = Dynamics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c78b66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def V(init_state, horizon):\n",
    "    gamma = 0.9\n",
    "    value = 0\n",
    "    state = init_state\n",
    "    for t in range(1, horizon):\n",
    "        action = policy.get_action(state)\n",
    "        reward = reward_function.calculate(state, action)\n",
    "        value += (gamma**t) * reward\n",
    "        state = dynamics.step(state, action)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44a5b1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_of_V(init_state, horizon): # TODO horizon не дан\n",
    "    N = 100\n",
    "    V_avg = sum([V(init_state, horizon) for _ in range(N)]) / N\n",
    "    return V_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0930622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65b2d952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eeaffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.json', 'r') as json_file:\n",
    "    train = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a57e1658",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes_number = len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0919a18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamics_train = []\n",
    "policy_train = []\n",
    "reward_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a921896b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4340369e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:08<00:00, 58.07it/s]\n"
     ]
    }
   ],
   "source": [
    "for episode in tqdm(range(500)):\n",
    "    episode_trajectory = train[episode]\n",
    "    states = episode_trajectory[\"states\"]\n",
    "    actions = episode_trajectory[\"actions\"]\n",
    "    rewards = episode_trajectory[\"rewards\"]\n",
    "    truncated = episode_trajectory[\"truncated\"]\n",
    "    N = len(rewards)\n",
    "    \n",
    "    \n",
    "    for t in range(N-1):\n",
    "        s = states[t]\n",
    "        a = actions[t]\n",
    "        r = rewards[t]\n",
    "        s_ = states[t+1]\n",
    "        \n",
    "        if t == 0:\n",
    "            s_a_s = utils.prepare_state_action(s, a)\n",
    "            s_s = utils.prepare_state(s)\n",
    "            a_s = np.array([a])\n",
    "            r_s = np.array([r])\n",
    "            s_n_s = utils.prepare_state(s_)\n",
    "        else:\n",
    "            s_a_s = np.vstack((s_a_s, utils.prepare_state_action(s, a)))\n",
    "            s_s = np.vstack((s_s, utils.prepare_state(s)))\n",
    "            a_s = np.vstack((a_s, np.array([a])))\n",
    "            r_s = np.vstack((r_s, np.array([r])))\n",
    "            s_n_s = np.vstack((s_n_s, utils.prepare_state(s_)))\n",
    "            \n",
    "        dynamics_train.append({\"states_actions\": s_a_s, \"next_states\": s_n_s})\n",
    "        policy_train.append({\"states\": s_s, \"actions\": a_s})\n",
    "        reward_train.append( {\"states_actions\": s_a_s, \"rewards\": r_s})\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "548f387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed02bab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c217888",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy()\n",
    "reward_function = Reward()\n",
    "dynamics = Dynamics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ede5a2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 57378/57378 [00:41<00:00, 1383.40it/s]\n"
     ]
    }
   ],
   "source": [
    "for b in tqdm(range(len(policy_train)//3)):\n",
    "    policy.fit(policy_train[b][\"states\"], policy_train[b][\"actions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70e27300",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 34427/34427 [00:24<00:00, 1384.84it/s]\n"
     ]
    }
   ],
   "source": [
    "for b in tqdm(range(len(reward_train)//5)):\n",
    "    reward_function.fit(reward_train[b][\"states_actions\"], reward_train[b][\"rewards\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6804ffc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 34427/34427 [00:25<00:00, 1325.66it/s]\n"
     ]
    }
   ],
   "source": [
    "for b in tqdm(range(len(dynamics_train)//5)):\n",
    "    dynamics.fit(dynamics_train[b][\"states_actions\"], dynamics_train[b][\"next_states\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bddd70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75382f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec01f9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44d7370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from random import random\n",
    "\n",
    "\n",
    "def main():\n",
    "    # считываем данные\n",
    "    with open('train.json', 'r') as json_file:\n",
    "        train = json.load(json_file)\n",
    "\n",
    "    # посмотрим на информацию, доступную в нулевом эпизоде\n",
    "    print(train[0].keys())\n",
    "\n",
    "    # напечатаем возможные действия\n",
    "    print(set(train[0]['actions']))\n",
    "\n",
    "    # считываем данные, для которых нужно сделать предсказание\n",
    "    with open('submit.json', 'r') as json_file:\n",
    "        submit = json.load(json_file)\n",
    "\n",
    "    # заполним поле V случайными числами\n",
    "    for idx, item in enumerate(submit):\n",
    "        state = item['state']\n",
    "        item['V'] = V(state, 100)\n",
    "\n",
    "    # записываем предсказания в файл\n",
    "    with open('solution.json', 'w') as json_file:\n",
    "        json.dump(submit, json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb590f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['states', 'rewards', 'actions', 'truncated'])\n",
      "{0, 1, 2}\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55aa8d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
