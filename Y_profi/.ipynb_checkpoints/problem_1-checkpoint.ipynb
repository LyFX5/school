{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-07T20:23:16.162241Z",
     "start_time": "2023-04-07T20:23:16.120067Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# открыть хабр про линейную регрессию и принцип максимального правдоподобия\n",
    "# взять имеющиеся конспекты\n",
    "# поискать в прошлогодних на винде задачу про дерево менеджмента и матожидание выгоды\n",
    "# теория по статистике курс на степике по статистике бутстрап\n",
    "# доверительные интервалы (конспект по статистике)\n",
    "# стандартные метрики\n",
    "# загатовка градиентного бустинга\n",
    "# загатовка эмбединга и выделения признаков из разносортных данных\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-07T15:26:40.967922Z",
     "start_time": "2023-04-07T15:26:21.048110Z"
    }
   },
   "outputs": [],
   "source": [
    "n = int(input())\n",
    "a = list(map(int, input().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-07T15:26:41.668747Z",
     "start_time": "2023-04-07T15:26:41.661463Z"
    }
   },
   "outputs": [],
   "source": [
    "def solve(a, left, right, dp):\n",
    "    if dp is None:\n",
    "        dp = {}\n",
    "    if left is None:\n",
    "        left = 0\n",
    "    if right is None:\n",
    "        right = len(a) - 1\n",
    "\n",
    "    if (left, right, a[left], a[right]) in dp:\n",
    "        return dp[(left, right, a[left], a[right])]\n",
    "\n",
    "    q = []\n",
    "    for i in range(left, right):\n",
    "        g = min(a[i], a[i+1])\n",
    "        if g > 0:\n",
    "            a[i] -= g\n",
    "            a[i+1] -= g\n",
    "\n",
    "            q.append(solve(a, left, i, dp) ^ solve(a, i+1, right, dp))\n",
    "\n",
    "            a[i] += g\n",
    "            a[i+1] += g\n",
    "    q = set(q)\n",
    "\n",
    "    mex = 0\n",
    "    while mex in q:\n",
    "        mex += 1\n",
    "    dp[(left, right, a[left], a[right])] = mex\n",
    "\n",
    "    return mex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-07T15:26:42.559367Z",
     "start_time": "2023-04-07T15:26:42.549919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(solve(a, 0, len(a)-1, {}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-07T15:26:47.975311Z",
     "start_time": "2023-04-07T15:26:47.945934Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "13 ^ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define regularization parameter\n",
    "lmbd = 0.1\n",
    "\n",
    "degree_list = [1, 2, 3, 10, 12, 13]\n",
    "cmap = plt.get_cmap('jet')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, len(degree_list))]\n",
    "\n",
    "margin = 0.3\n",
    "plt.plot(data['support'], data['values'], 'b--', alpha=0.5, label='manifold')\n",
    "plt.scatter(data['x_train'], data['y_train'], 40, 'g', 'o', alpha=0.8, label='data')\n",
    "\n",
    "w_list_l2 = []\n",
    "err = []\n",
    "for ix, degree in enumerate(degree_list):\n",
    "    dlist = [[1]*data['x_train'].shape[0]] + map(lambda n: data['x_train']**n, range(1, degree + 1))\n",
    "    X = np.array(dlist).T\n",
    "    w = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X) + lmbd*np.eye(X.shape[1])), X.T), data['y_train'])\n",
    "    w_list_l2.append((degree, w))\n",
    "    y_hat = np.dot(w, X.T)\n",
    "    plt.plot(data['x_train'], y_hat, color=colors[ix], label='poly degree: %i' % degree)\n",
    "    err.append(np.mean((data['y_train'] - y_hat)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=17)\n",
    "forest.fit(X_train, y_train)\n",
    "print(round(forest.score(X_test, y_test), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear models \n",
    "import logging\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except ImportError:\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "from mla.linear_models import LinearRegression, LogisticRegression\n",
    "from mla.metrics.metrics import mean_squared_error, accuracy\n",
    "\n",
    "# Change to DEBUG to see convergence\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "\n",
    "def regression():\n",
    "    # Generate a random regression problem\n",
    "    X, y = make_regression(\n",
    "        n_samples=10000, n_features=100, n_informative=75, n_targets=1, noise=0.05, random_state=1111, bias=0.5\n",
    "    )\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1111)\n",
    "\n",
    "    model = LinearRegression(lr=0.01, max_iters=2000, penalty=\"l2\", C=0.03)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    print(\"regression mse\", mean_squared_error(y_test, predictions))\n",
    "\n",
    "\n",
    "def classification():\n",
    "    # Generate a random binary classification problem.\n",
    "    X, y = make_classification(\n",
    "        n_samples=1000, n_features=100, n_informative=75, random_state=1111, n_classes=2, class_sep=2.5\n",
    "    )\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1111)\n",
    "\n",
    "    model = LogisticRegression(lr=0.01, max_iters=500, penalty=\"l1\", C=0.01)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    print(\"classification accuracy\", accuracy(y_test, predictions))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    regression()\n",
    "    classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from mla.kmeans import KMeans\n",
    "\n",
    "\n",
    "def kmeans_example(plot=False):\n",
    "    X, y = make_blobs(centers=4, n_samples=500, n_features=2, shuffle=True, random_state=42)\n",
    "    clusters = len(np.unique(y))\n",
    "    k = KMeans(K=clusters, max_iters=150, init=\"++\")\n",
    "    k.fit(X)\n",
    "    k.predict()\n",
    "\n",
    "    if plot:\n",
    "        k.plot()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    kmeans_example(plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive bayes\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from mla.naive_bayes import NaiveBayesClassifier\n",
    "\n",
    "\n",
    "def classification():\n",
    "    # Generate a random binary classification problem.\n",
    "    X, y = make_classification(\n",
    "        n_samples=1000, n_features=10, n_informative=10, random_state=1111, n_classes=2, class_sep=2.5, n_redundant=0\n",
    "    )\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1111)\n",
    "\n",
    "    model = NaiveBayesClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)[:, 1]\n",
    "\n",
    "    print(\"classification accuracy\", roc_auc_score(y_test, predictions))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nearest_neighbors\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except ImportError:\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets import make_regression\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from mla import knn\n",
    "from mla.metrics.metrics import mean_squared_error, accuracy\n",
    "\n",
    "\n",
    "def regression():\n",
    "    # Generate a random regression problem\n",
    "    X, y = make_regression(\n",
    "        n_samples=500, n_features=5, n_informative=5, n_targets=1, noise=0.05, random_state=1111, bias=0.5\n",
    "    )\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1111)\n",
    "\n",
    "    model = knn.KNNRegressor(k=5, distance_func=distance.euclidean)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    print(\"regression mse\", mean_squared_error(y_test, predictions))\n",
    "\n",
    "\n",
    "def classification():\n",
    "    X, y = make_classification(\n",
    "        n_samples=500,\n",
    "        n_features=5,\n",
    "        n_informative=5,\n",
    "        n_redundant=0,\n",
    "        n_repeated=0,\n",
    "        n_classes=3,\n",
    "        random_state=1111,\n",
    "        class_sep=1.5,\n",
    "    )\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1111)\n",
    "\n",
    "    clf = knn.KNNClassifier(k=5, distance_func=distance.euclidean)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "    print(\"classification accuracy\", accuracy(y_test, predictions))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    regression()\n",
    "    classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except ImportError:\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from mla.ensemble.random_forest import RandomForestClassifier, RandomForestRegressor\n",
    "from mla.metrics.metrics import mean_squared_error\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "\n",
    "def classification():\n",
    "    # Generate a random binary classification problem.\n",
    "    X, y = make_classification(\n",
    "        n_samples=500, n_features=10, n_informative=10, random_state=1111, n_classes=2, class_sep=2.5, n_redundant=0\n",
    "    )\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=1111)\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=10, max_depth=4)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    predictions_prob = model.predict(X_test)[:, 1]\n",
    "    predictions = np.argmax(model.predict(X_test), axis=1)\n",
    "    #print(predictions.shape)\n",
    "    print(\"classification, roc auc score: %s\" % roc_auc_score(y_test, predictions_prob))\n",
    "    print(\"classification, accuracy score: %s\" % accuracy_score(y_test, predictions))\n",
    "\n",
    "\n",
    "def regression():\n",
    "    # Generate a random regression problem\n",
    "    X, y = make_regression(\n",
    "        n_samples=500, n_features=5, n_informative=5, n_targets=1, noise=0.05, random_state=1111, bias=0.5\n",
    "    )\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1111)\n",
    "\n",
    "    model = RandomForestRegressor(n_estimators=50, max_depth=10, max_features=3)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    print(\"regression, mse: %s\" % mean_squared_error(y_test.flatten(), predictions.flatten()))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    classification()\n",
    "    # regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm\n",
    "import logging\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except ImportError:\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from mla.metrics.metrics import accuracy\n",
    "from mla.svm.kernerls import Linear, RBF\n",
    "from mla.svm.svm import SVM\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "\n",
    "def classification():\n",
    "    # Generate a random binary classification problem.\n",
    "    X, y = make_classification(\n",
    "        n_samples=1200, n_features=10, n_informative=5, random_state=1111, n_classes=2, class_sep=1.75\n",
    "    )\n",
    "    # Convert y to {-1, 1}\n",
    "    y = (y * 2) - 1\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1111)\n",
    "\n",
    "    for kernel in [RBF(gamma=0.1), Linear()]:\n",
    "        model = SVM(max_iter=500, kernel=kernel, C=0.6)\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        print(\"Classification accuracy (%s): %s\" % (kernel, accuracy(y_test, predictions)))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gausian mixture\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from mla.kmeans import KMeans\n",
    "from mla.gaussian_mixture import GaussianMixture\n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(6)\n",
    "\n",
    "\n",
    "def make_clusters(skew=True, *arg, **kwargs):\n",
    "    X, y = datasets.make_blobs(*arg, **kwargs)\n",
    "    if skew:\n",
    "        nrow = X.shape[1]\n",
    "        for i in np.unique(y):\n",
    "            X[y == i] = X[y == i].dot(np.random.random((nrow, nrow)) - 0.5)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def KMeans_and_GMM(K):\n",
    "    COLOR = \"bgrcmyk\"\n",
    "\n",
    "    X, y = make_clusters(skew=True, n_samples=1500, centers=K)\n",
    "    _, axes = plt.subplots(1, 3)\n",
    "\n",
    "    # Ground Truth\n",
    "    axes[0].scatter(X[:, 0], X[:, 1], c=[COLOR[int(assignment)] for assignment in y])\n",
    "    axes[0].set_title(\"Ground Truth\")\n",
    "\n",
    "    # KMeans\n",
    "    kmeans = KMeans(K=K, init=\"++\")\n",
    "    kmeans.fit(X)\n",
    "    kmeans.predict()\n",
    "    axes[1].set_title(\"KMeans\")\n",
    "    kmeans.plot(ax=axes[1], holdon=True)\n",
    "\n",
    "    # Gaussian Mixture\n",
    "    gmm = GaussianMixture(K=K, init=\"kmeans\")\n",
    "    gmm.fit(X)\n",
    "    axes[2].set_title(\"Gaussian Mixture\")\n",
    "    gmm.plot(ax=axes[2])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    KMeans_and_GMM(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep q learning\n",
    "import logging\n",
    "\n",
    "from mla.neuralnet import NeuralNet\n",
    "from mla.neuralnet.layers import Activation, Dense\n",
    "from mla.neuralnet.optimizers import Adam\n",
    "from mla.rl.dqn import DQN\n",
    "\n",
    "logging.basicConfig(level=logging.CRITICAL)\n",
    "\n",
    "\n",
    "def mlp_model(n_actions, batch_size=64):\n",
    "    model = NeuralNet(\n",
    "        layers=[Dense(32), Activation(\"relu\"), Dense(n_actions)],\n",
    "        loss=\"mse\",\n",
    "        optimizer=Adam(),\n",
    "        metric=\"mse\",\n",
    "        batch_size=batch_size,\n",
    "        max_epochs=1,\n",
    "        verbose=False,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model = DQN(n_episodes=2500, batch_size=64)\n",
    "model.init_environment(\"CartPole-v0\")\n",
    "model.init_model(mlp_model)\n",
    "\n",
    "try:\n",
    "    # Train the model\n",
    "    # It can take from 300 to 2500 episodes to solve CartPole-v0 problem due to randomness of environment.\n",
    "    # You can stop training process using Ctrl+C signal\n",
    "    # Read more about this problem: https://gym.openai.com/envs/CartPole-v0\n",
    "    model.train(render=False)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "# Render trained model\n",
    "model.play(episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
